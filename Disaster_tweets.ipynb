{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disaster_tweets.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwlhCKg6tzXF"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n",
        "\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv('/content/train.csv')\n",
        "test_df=pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "id": "Rh6pH2UeudId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disaster_tweets = train_df[train_df['target']==1]['text']\n",
        "disaster_tweets.values[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DHAY7T5avnnU",
        "outputId": "965fe3cf-30ee-45ec-ebbd-82f0d8414468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Forest fire near La Ronge Sask. Canada'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disaster_tweets = train_df[train_df['target']==0]['text']\n",
        "disaster_tweets.values[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jszMBa08w_ns",
        "outputId": "94cc1ad8-9751-4fa0-967d-3800891e7b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I love fruits'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "zR8bd0vCxB75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text'] = train_df['text'].apply(lambda x: clean_text(x))\n",
        "test_df['text'] = test_df['text'].apply(lambda x : clean_text(x))"
      ],
      "metadata": {
        "id": "ft--WeDDxhZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "train_df['text'] = train_df['text'].apply(lambda x: tokenizer.tokenize(x))\n",
        "test_df['text'] = test_df['text'].apply(lambda x: tokenizer.tokenize(x))"
      ],
      "metadata": {
        "id": "FyIfVW4wxkL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MunJhsply5-r",
        "outputId": "23caa002-1cd4-4ee9-c054-60f4587cc156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    words = [w for w in text if w not in stopwords.words('english')]\n",
        "    return words\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(lambda x : remove_stopwords(x))\n",
        "test_df['text'] = test_df['text'].apply(lambda x : remove_stopwords(x))"
      ],
      "metadata": {
        "id": "gNo_SsDRyn8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_text(list_of_text):\n",
        "    combined_text = ' '.join(list_of_text)\n",
        "    return combined_text\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(lambda x : combine_text(x))\n",
        "test_df['text'] = test_df['text'].apply(lambda x : combine_text(x))"
      ],
      "metadata": {
        "id": "J9GotcZOzX7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(text):\n",
        "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "    \n",
        "    nopunc = clean_text(text)\n",
        "    tokenized_text = tokenizer.tokenize(nopunc)\n",
        "    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
        "    combined_text = ' '.join(remove_stopwords)\n",
        "    return combined_text"
      ],
      "metadata": {
        "id": "mn7X_FvUzhk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "train_df_vectors = count_vectorizer.fit_transform(train_df['text'])\n",
        "test_df_vectors = count_vectorizer.transform(test_df[\"text\"])\n",
        "\n",
        "## Keeping only non-zero elements to preserve space \n",
        "print(train_df_vectors[0].todense())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mblT87hYzjnu",
        "outputId": "69b6f27d-fc39-4600-fd6e-98343a0b73d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF Features (Term Frequency-Inverse Document Frequency, or TF-IDF for short)\n",
        "\n",
        "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
        "train_df_tfidf = tfidf.fit_transform(train_df['text'])\n",
        "test_df_tfidf = tfidf.transform(test_df[\"text\"])"
      ],
      "metadata": {
        "id": "1uTqMG1t0QzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting a simple Logistic Regression on Counts\n",
        "clf = LogisticRegression(C=1.0)\n",
        "scores = model_selection.cross_val_score(clf, train_df_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nAFNiT80XPo",
        "outputId": "51e236f5-5b5e-435d-ee56-9827441892fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.59865255, 0.49611063, 0.57166948, 0.56290774, 0.68789809])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(train_df_vectors, train_df[\"target\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSmf9Xar0u4-",
        "outputId": "ff2ba17b-200e-4a63-f5fd-a9e89fa4afad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting a simple Logistic Regression on TFIDF\n",
        "clf_tfidf = LogisticRegression(C=1.0)\n",
        "scores = model_selection.cross_val_score(clf_tfidf, train_df_tfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzDo5uJm01EJ",
        "outputId": "19d673da-fed6-4ba5-e5ad-2423f5a431e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.57229525, 0.49673203, 0.54277829, 0.46618106, 0.64768683])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting a simple Naive Bayes on Counts\n",
        "\n",
        "clf_NB = MultinomialNB()\n",
        "scores = model_selection.cross_val_score(clf_NB, train_df_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRSPNY_W04n7",
        "outputId": "b6c149ee-8b8d-4c8c-e243-d34b2834b88c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.63149079, 0.60675773, 0.68575519, 0.64341085, 0.72505092])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting a simple Naive Bayes on TFIDF\n",
        "\n",
        "clf_NB_TFIDF = MultinomialNB()\n",
        "scores = model_selection.cross_val_score(clf_NB_TFIDF, train_df_tfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siDJ0FMS0_iB",
        "outputId": "692af26d-1923-483f-9aea-f09d9782b446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.57590597, 0.57092511, 0.61135371, 0.5962963 , 0.7393745 ])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "clf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
        "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "scores = model_selection.cross_val_score(clf_xgb, train_df_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV-c2cTJ1CFV",
        "outputId": "03a72c01-66f5-43bb-ce94-37b523c7a0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.47379913, 0.37379576, 0.43988816, 0.38900634, 0.53142857])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "clf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
        "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "scores = model_selection.cross_val_score(clf_xgb_TFIDF, train_df_tfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDM3vfAf1F1c",
        "outputId": "ba8610a8-0921-4741-d76d-9eb19f8a8df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.48947951, 0.34406439, 0.43140965, 0.40084388, 0.53014354])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(test_df_vectors)\n",
        "y_pred = np.round(y_pred).astype(int).reshape(3263)\n",
        "\n",
        "sub = pd.DataFrame({'id': test_df['id'].values.tolist(),'target' : y_pred})\n",
        "\n",
        "sub.to_csv('submission.csv',index = False)"
      ],
      "metadata": {
        "id": "0iqYPTiw1g6c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}